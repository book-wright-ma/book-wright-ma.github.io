<HTML>
  
<HEAD>
  <TITLE> Lecture Slides for the Book of Wright and Ma</TITLE>  
</HEAD>

  <BODY>  

<center>
  <h2> Lecture Slides for Berkeley EECS 208, Fall 2021</h2>
  <h2> by Yi Ma </h2>
</center>

<b>
<ul>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_1.pdf"> Lecture 1</a>: Introduction. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_2.pdf"> Lecture 2</a>: Sparse Models. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_3.pdf"> Lecture 3</a>: Relaxing the Sparse Recovery Problem. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_4.pdf"> Lecture 4</a>: Convex Methods for Sparse Recovery. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_5.pdf"> Lecture 5</a>: Convex Methods: Towards Stronger Correctness Results. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_6.pdf"> Lecture 6</a>: Convex Methods: Matrices with Restricted Isometry Property. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_7.pdf"> Lecture 7</a>: Convex Methods: Noisy Observations and Approximate Sparsity. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_8.pdf"> Lecture 8</a>: Convex Methods: Phase Transition in Sparse Recovery. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_9.pdf"> Lecture 9</a>: Low-Rank Matrix Recovery: Random Measurements. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_10.pdf"> Lecture 10</a>: Low-Rank Matrix Recovery: Matrix Completion. <br>  
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_11.pdf"> Lecture 11</a>: Decomposing Low-Rank and Sparse: Principal Component Pursuit. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_13.pdf"> Lecture 13</a>: Convex Optimization for Structured Data: Unconstrained. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_14.pdf"> Lecture 14</a>: Convex Optimization for Structured Data: Constrained. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_15.pdf"> Lecture 15</a>: Nonconvex Methods: Dictionary Learning. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_16.pdf"> Lecture 16</a>: Nonconvex Methods: Dictionary Learning via L4 Maximization. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_17.pdf"> Lecture 17</a>: Nonconvex Optimization: First Order Methods. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_18.pdf"> Lecture 18</a>: Nonconvex Optimization: Power Iteration and Fixed Point. <br>  
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_19.pdf"> Lecture 19</a>: Structured Nonlinear Models: Sparsity in Convolution and Deconvolution. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_20.pdf"> Lecture 20</a>: Structured Nonlinear Models: Transform Invariant Low-Rank Texture. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_21_22.pdf"> Lecture 21 and 22</a>: Deep Models: White-Box Deep (Convolution) Networks from
the Principle of Rate Reduction. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_23.pdf"> Lecture 23</a>: Deep Models: Closed-Loop Data Transcription to Low-dim Models via Minimaxing Rate Reduction. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/2018April-MaYi.pdf"> A comprehensive presentation on low-rank models</a>. <br>  
</ul>
    </b>  
 
<p>
  <b>  Note:</b> slides for Lecture 12 (about analysis and proof of Principal Component Pursuit) are missing as they had not been made. We will add them later.
    </p>
  </BODY>
</HTML>
