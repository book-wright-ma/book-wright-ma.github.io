<HTML>
  
<HEAD>
  <TITLE> Lecture Slides for the Book of Wright and Ma </TITLE>  
</HEAD>

  <BODY>  

<center>
  <h3> Lecture Slides for <a href="https://book-wright-ma.github.io">the Book of Wright and Ma</a></h3>
  <h3> by Yi Ma (for <a href="https://pages.github.berkeley.edu/UCB-EECS208/course_site/">Berkeley EECS208, Fall 2021</a>)</h3>
</center>

<p>
  <b> Guidelines</b>: The lectures are for a four-unit one-semester (fourteen weeks) course and each lecture is for one and a half hours. The lectures follow generally the same order of the book chapters. They are intentionally organized into several modules, so that it would be easier for future teachers to customize them to 
  courses with different focus, length, and depth.
</p>

<p>
<i> Be prepared for a journey from high-dimensional to low-dimensional, from convex to nonconvex; from linear to nonlinear; from shallow to deep; from idealistic to realistic; and from theoretical to practical. 
Or at a more technical and computational level, this is also a journey from L0, to L1, and to L4; from low-rank, to nuclear norm, and to log-det; and from 
  PCA, to GPCA, to Nonlinear PCAs.</i> 
</p>    
<b>
<ul>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_1.pdf"> Lecture 01</a>: Introduction: Background, History, and Overview. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_2.pdf"> Lecture 02</a>: Sparse Models and L0 Minimization. <br> 
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_3.pdf"> Lecture 03</a>: Relaxing the Sparse Recovery Problem via L1 Minimization. <br><br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_4.pdf"> Lecture 04</a>: Convex Methods for Correct Sparse Recovery. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_5.pdf"> Lecture 05</a>: Convex Sparse Recovery: Towards Stronger Correctness Results. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_6.pdf"> Lecture 06</a>: Convex Sparse Recovery: Matrices with Restricted Isometry Property. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_7.pdf"> Lecture 07</a>: Convex Sparse Recovery: Noisy Observations and Approximate Sparsity. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_8.pdf"> Lecture 08</a>: Convex Sparse Recovery: Phase Transition in Sparse Recovery. <br><br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_9.pdf"> Lecture 09</a>: Convex Low-Rank Matrix Recovery: Random Measurements. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_10.pdf"> Lecture 10</a>: Convex Low-Rank Matrix Recovery: Matrix Completion. <br>  
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_11.pdf"> Lecture 11</a>: Convex Low-Rank and Sparse Decomposition: Algorithms. <br>
  <li>  Lecture 12: Convex Low-Rank and Sparse Decomposition: Analysis and Proof. <br><br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_13.pdf"> Lecture 13</a>: Convex Optimization for Structured Data: Unconstrained. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_14.pdf"> Lecture 14</a>: Convex Optimization for Structured Data: Constrained & Scalable. <br><br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_15.pdf"> Lecture 15</a>: Nonconvex Formulations: Dictionary Learning. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_16.pdf"> Lecture 16</a>: Nonconvex Methods: Dictionary Learning via L4 Maximization. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_17.pdf"> Lecture 17</a>: Nonconvex Optimization: First Order Methods. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_18.pdf"> Lecture 18</a>: Nonconvex Optimization: Power Iteration and Fixed Point. <br><br>  
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_19.pdf"> Lecture 19</a>: Nonlinear Structured Models: Sparsity in Convolution and Deconvolution. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_20.pdf"> Lecture 20</a>: Nonlinear Structured Models: Transform Invariant Low-Rank Texture. <br><br> 
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_21_22.pdf"> Lecture 21</a>: Deep Discriminative Models: The Principle of Maximal Coding Rate Reduction. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_21_22.pdf"> Lecture 22</a>: Deep Discriminative Models: White-Box Deep Convolution Networks from Rate Reduction. <br>  
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_23.pdf"> Lecture 23</a>: Deep Generative Models: Closed-Loop Data Transcription via Minimaxing Rate Reduction. <br>
</ul>
    </b>  
 
<p>
  <b>  Notes:</b> Lectures 21 and 22 share the same (long) deck of slides. Slides for Lecture 12, about analysis and proof of Principal Component Pursuit, 
  are unavailable as they were not made when the course was offered. 
  We will add them in the future as well as keep slides for all the lectures updated. (<i>This version is last updated on December 15, 2021.</i>)  
</p>
    
    <p> <b> Additional notes for discussion sessions of the course</b> (administered by teaching assisant Simon Zhai):</p>
<ul> 
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Discussion_1.pdf"> Discussion 1</a>: Linear Algebra and Statistics.<br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Discussion_2.pdf"> Discussion 2</a>: Facts from High-dim Statistics. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Discussion_3.pdf"> Discussion 3</a>: Facts from Matrix Analysis. <br>  
</ul>
    <p> <b> Additional presentations and guest lectures</b> (by the teaching staff or colleagues):</p>
<ul>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/2018April-MaYi.pdf">Low-Dimensional Models: Algorithms and Applications</a> by Yi Ma. <br> 
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/YuxinChen_slides.pdf">Bridging Convex and Nonconvex Optimization in
          Noisy Matrix Completion</a> by Professor Yuxin Chen of Princeton University.<br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/QingQu_slides.pdf">From Shallow to Deep Representation Learning: Global Nonconvex Theory and Algorithms<a>
       by Professor Qing Qu of University of Michigan.<br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/YuqianZhang_slides.pdf">Sparse Blind Deconvolution: Nonconvex Geometry and Algorithm</a> 
       by Professor Yuqian Zhang of Rutgers University.<br> 
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/MertPilanci_slides.pdf">The Hidden Convex Optimization Landscape of Deep Neural Networks</a> 
       by Professor Mert Pilanci of Stanford University.<br> 
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/JohnWright_slides.pdf">Deep Networks and the Multiple Manifold Problem</a> by Professor John Wright of Columbia University.<br>  
</ul>
    
</BODY>
</HTML>
