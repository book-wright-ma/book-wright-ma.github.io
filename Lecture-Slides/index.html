<HTML>
  
<HEAD>
  <TITLE> Lecture Slides for the Book of Wright and Ma</TITLE>  
</HEAD>

  <BODY>  

<center>
  <h2> Lecture Slides for Berkeley EECS 208, Fall 2021</h2>
  <h2> by Yi Ma </h2>
</center>

<b>
<ul>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_1.pdf"> Lecture_1</a>: Introduction. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_2.pdf"> Lecture_2</a>: Sparse Models. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_3.pdf"> Lecture_3</a>: Relaxing the Sparse Recovery Problem. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_4.pdf"> Lecture_4</a>: Convex Methods for Sparse Recovery. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_5.pdf"> Lecture_5</a>: Convex Methods: Towards Stronger Correctness Results. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_6.pdf"> Lecture_6</a>: Convex Methods: Matrices with Restricted Isometry Property. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_7.pdf"> Lecture_7</a>: Convex Methods: Noisy Observations and Approximate Sparsity. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_8.pdf"> Lecture_8</a>: Convex Methods: Phase Transition in Sparse Recovery. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_9.pdf"> Lecture_9</a>: Low-Rank Matrix Recovery: Random Measurements. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_10.pdf"> Lecture_10</a>: Low-Rank Matrix Recovery: Matrix Completion. <br>  
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_11.pdf"> Lecture_11</a>: Decomposing Low-Rank and Sparse: Principal Component Pursuit. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_13.pdf"> Lecture_13</a>: Unconstrained Convex Optimization for Structured Data. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_14.pdf"> Lecture_14</a>: Constrained and Scalable Convex Optimization for Structured Data. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_15.pdf"> Lecture_15</a>: Nonconvex Methods: Dictionary Learning. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_16.pdf"> Lecture_16</a>: Nonconvex Methods: Dictionary Learning via L4 Maximization. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_17.pdf"> Lecture_17</a>: Nonconvex Optimization: First Order Methods. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_18.pdf"> Lecture_18</a>: Nonconvex Optimization: Power Iteration and Fixed Point. <br>  
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_19.pdf"> Lecture_19</a>: Structured Nonlinear Models: Sparsity in Convolution and Deconvolution. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_20.pdf"> Lecture_20</a>: Structured Nonlinear Models: Transform Invariant Low-Rank Texture. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_21_22.pdf"> Lecture_21 and 22</a>: ReduNet: White-Box Deep (Convolution) Networks from
the Principle of Rate Reduction. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_23.pdf"> Lecture_23</a>: Closed-Loop Data Transcription to an LDR via Minimaxing Rate Reduction. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/2018April-MaYi.pdf"> A comprehensive presentation on low-rank models</a>. <br>  
</ul>
    </b>  
 
<p>
  <b>  Note:</b> slides for Lecture_12 (about analysis and proof of Principal Component Pursuit) are missing as they had not been made. We will add them later.
    </p>
  </BODY>
</HTML>
