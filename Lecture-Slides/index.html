<HTML>
  
<HEAD>
  <TITLE> Lecture Slides for the Book of Wright and Ma </TITLE>  
</HEAD>

  <BODY>  

<center>
  <table>
    <tr>
      <td> <img width=228 align=middle src="book-organization.png" alt="Book Cover"> 
      </td>
      <td>  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
      </td>
      <td>
         
 <center>
  <h3> <font face="Arial,Helvetica"> Lecture Slides for <a href="https://book-wright-ma.github.io">the Book of Wright and Ma</a> </font></h3>
  <h3> <font face="Arial,Helvetica"> by Yi Ma (for <a href="https://pages.github.berkeley.edu/UCB-EECS208/course_site/">Berkeley EECS208, Fall 2021</a>) </font> </h3>
  </center>
      </td>
    </tr>
  </table>
    </center>
<p>
  <b> Guidelines</b>: The lectures are for a four-unit, one-semester, fourteen-week course, and each lecture is for one and a half hours. The lectures follow 
  generally the same order of the book chapters. They are intentionally organized into several modules, so that future instructors can easily customize them to 
  courses with different focus, length, and depth. Please contact <a href="https://people.eecs.berkeley.edu/~yima/">Yi Ma</a> 
  if any instructors like to obtain latex files of the slides editable for their own lectures.
</p>

<p>
<i> Now, be prepared for a journey from high-dimensional to low-dimensional, from linear to nonlinear, from convex to nonconvex, from shallow to deep, from idealistic to realistic, and from theoretical to practical. 
Or, at a more technical and computational level, this is also a journey from L0, to L1, and to L4, from rank, to nuclear norm, and to coding rate, and from 
  one subspace, to multiple subspaces, and to multiple submanifolds.</i> 
</p>    
<b>
<ul>
  <font face="Arial,Helvetica"> Chapter 1 and 2:</font>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_1.pdf"> Lecture 01</a>: Introduction: Background, History, and Overview. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_2.pdf"> Lecture 02</a>: Sparse Models and L0 Minimization. <br> 
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_3.pdf"> Lecture 03</a>: Relaxing the Sparse Recovery Problem via L1 Minimization. <br><br>
  <font face="Arial,Helvetica"> Chapter 3 and 6:</font>  
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_4.pdf"> Lecture 04</a>: Convex Methods for Correct Sparse Recovery. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_5.pdf"> Lecture 05</a>: Convex Sparse Recovery: Towards Stronger Correctness Results. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_6.pdf"> Lecture 06</a>: Convex Sparse Recovery: Matrices with Restricted Isometry Property. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_7.pdf"> Lecture 07</a>: Convex Sparse Recovery: Noisy Observations and Approximate Sparsity. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_8.pdf"> Lecture 08</a>: Convex Sparse Recovery: Phase Transition in Sparse Recovery. <br><br>
  <font face="Arial,Helvetica"> Chapter 4 and 5:</font>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_9.pdf"> Lecture 09</a>: Convex Low-Rank Matrix Recovery: Random Measurements. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_10.pdf"> Lecture 10</a>: Convex Low-Rank Matrix Recovery: Matrix Completion. <br>  
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_11.pdf"> Lecture 11</a>: Convex Low-Rank and Sparse Decomposition: Algorithms. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_12_Jiao.pdf"> Lecture 12A</a>: Convex Low-Rank and Sparse Decomposition: Analysis (jamboard notes by Jiantao Jiao). <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_12.pdf"> Lecture 12B</a>: Convex Low-Rank and Sparse Decomposition: Extentions. <br><br>
  <font face="Arial,Helvetica"> Chapter 8:</font>  
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_13.pdf"> Lecture 13</a>: Convex Optimization for Structured Data: Unconstrained. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_14.pdf"> Lecture 14</a>: Convex Optimization for Structured Data: Constrained & Scalable. <br><br>
  <font face="Arial,Helvetica"> Chapter 7 and 9:</font>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_15.pdf"> Lecture 15</a>: Nonconvex Formulations: Sparsifying Dictionary Learning. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_16.pdf"> Lecture 16</a>: Nonconvex Methods: Dictionary Learning via L4 Maximization. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_17.pdf"> Lecture 17</a>: Nonconvex Optimization: First Order Methods. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_18.pdf"> Lecture 18</a>: Nonconvex Optimization: Power Iteration and Fixed Point. <br><br>  
  <font face="Arial,Helvetica"> Chapter 12 and 15:</font>  
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_19.pdf"> Lecture 19</a>: Nonlinear Structured Models: Sparsity in Convolution and Deconvolution. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_20.pdf"> Lecture 20</a>: Nonlinear Structured Models: Transform Invariant Low-Rank Texture. <br><br> 
  <font face="Arial,Helvetica"> Chapter 16 and beyond:</font>  
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_21_22.pdf"> Lecture 21</a>: Deep Discriminative Models: The Principle of Maximal Coding Rate Reduction. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_21_22.pdf"> Lecture 22</a>: Deep Discriminative Models: White-Box Deep Convolution Networks from Rate Reduction. <br>  
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Lecture_23.pdf"> Lecture 23</a>: Deep Generative Models: Closed-Loop Data Transcription via Minimaxing Rate Reduction. <br>
</ul>
    </b>  
 
<p>
  <b>  Notes:</b>  Slides for Lecture 12A, about analysis and proof of Principal Component Pursuit, 
  are based on jamboard notes from Professor Jiantao Jiao. Lecture 12B is about extentions and generalizations to PCP. Lectures 21 and 22 share the same (long) deck of slides. 
  (<i>This version was last updated on <b>2021-12-28</b>.</i>)   
</p>
    
    <p> <b> Additional notes for discussion sessions of the course</b> (administered by teaching assisant Simon Zhai):</p>
<ul> 
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Discussion_1.pdf"> Discussion 1</a>: Linear Algebra and Statistics.<br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Discussion_2.pdf"> Discussion 2</a>: Facts from High-dim Statistics. <br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/Discussion_3.pdf"> Discussion 3</a>: Facts from Matrix Analysis. <br>  
</ul>
    <p> <b> Additional presentations and guest lectures</b> (by the teaching staff or colleagues):</p>
<ul>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/2018April-MaYi.pdf">Low-Dimensional Models: Algorithms and Applications</a> by Yi Ma. <br> 
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/YuxinChen_slides.pdf">Bridging Convex and Nonconvex Optimization in
          Noisy Matrix Completion</a> by Professor Yuxin Chen of Princeton University.<br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/QingQu_slides.pdf">From Shallow to Deep Representation Learning: Global Nonconvex Theory and Algorithms<a>
       by Professor Qing Qu of University of Michigan.<br>
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/YuqianZhang_slides.pdf">Sparse Blind Deconvolution: Nonconvex Geometry and Algorithm</a> 
       by Professor Yuqian Zhang of Rutgers University.<br> 
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/MertPilanci_slides.pdf">The Hidden Convex Optimization Landscape of Deep Neural Networks</a> 
       by Professor Mert Pilanci of Stanford University.<br> 
  <li> <a href="https://book-wright-ma.github.io/Lecture-Slides/JohnWright_slides.pdf">Deep Networks and the Multiple Manifold Problem</a> by Professor John Wright of Columbia University.<br>  
</ul>

<hr>
<font size=-1>&copy;2021 <a href="http://people.eecs.berkeley.edu/~yima/">Yi Ma</a><br>
<!-- Created: Thu December 16 20:19:00 PDT -->
<!-- hhmts start -->Last modified: Thur December 16 11:15:08 PST 2021 <!-- hhmts end -->    
    
</BODY>
</HTML>
